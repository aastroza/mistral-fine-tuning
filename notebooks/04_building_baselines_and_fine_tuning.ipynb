{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "import json\n",
    "import weave\n",
    "import pandas as pd\n",
    "import time\n",
    "from mistralai.async_client import MistralAsyncClient\n",
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.jobs import TrainingParameters, WandbIntegrationIn\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralAsyncClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "weave.init(\"mistral_hackathon\")\n",
    "\n",
    "@weave.op()\n",
    "async def call_mistral(model:str, messages:list, **kwargs) -> str:\n",
    "    \"Call the Mistral API\"\n",
    "    chat_response = await client.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(keyword: str, cls=ChatMessage):\n",
    "    messages = [\n",
    "        cls(\n",
    "            role=\"user\", \n",
    "            content=(\n",
    "                \"You are a world-class comedy writer specializing in Chilean humor.\"\n",
    "                \"You will write a joke in Chilean Spanish based on the keyword provided by the user.\"\n",
    "                \"Only output the joke, ignore any other explanation or context.\"\n",
    "                \"Write in Chilean Spanish.\"\n",
    "                 )\n",
    "        ),\n",
    "        cls(\n",
    "            role=\"assistant\", \n",
    "            content=(\n",
    "                \"Sure, I'd be happy to help writing a new joke in Chilean Spanish.\")\n",
    "        ),\n",
    "        cls(\n",
    "            role=\"user\", \n",
    "            content=f\"Write a joke in Chilean Spanish based on the following keyword: {keyword}.\"\n",
    "        )\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op()\n",
    "async def humor_writer(keyword:str, model:str) -> str:\n",
    "    \"Write a new joke\"\n",
    "     \n",
    "    messages = create_messages(keyword=keyword)\n",
    "\n",
    "    joke = await call_mistral(model=model, messages=messages)\n",
    "    return {\"keyword\": keyword, \"joke\": joke}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval = weave.ref('ds_eval:latest').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = await humor_writer(keyword=ds_eval.rows[0]['keyword'], model=\"mistral-medium-latest\")\n",
    "print(ds_eval.rows[0]['keyword'])\n",
    "print(res[\"joke\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MistralModel(weave.Model):\n",
    "    model: str\n",
    "    temperature: float = 0.7\n",
    "    \n",
    "    @weave.op\n",
    "    def create_messages(self, keyword:str):\n",
    "        return create_messages(keyword)\n",
    "\n",
    "    @weave.op\n",
    "    async def predict(self, keyword:str):\n",
    "        messages = self.create_messages(keyword)\n",
    "        return await call_mistral(model=self.model, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_medium = MistralModel(model=\"mistral-medium-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_foreach(sequence, func, max_concurrent_tasks):\n",
    "    \"Handy parallelism async for looper\"\n",
    "    semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
    "    async def process_item(item):\n",
    "        async with semaphore:\n",
    "            result = await func(item)\n",
    "            return item, result\n",
    "\n",
    "    tasks = [asyncio.create_task(process_item(item)) for item in sequence]\n",
    "\n",
    "    for task in asyncio.as_completed(tasks):\n",
    "        item, result = await task\n",
    "        yield item, result\n",
    "        \n",
    "async def map(ds, func, max_concurrent_tasks = 7, col_name=\"model_preds\"):\n",
    "    new_dataset = []\n",
    "    async for example, map_results in async_foreach(ds.rows, func, max_concurrent_tasks):\n",
    "        example.update({col_name: map_results})\n",
    "        new_dataset.append(example)\n",
    "    return new_dataset\n",
    "\n",
    "ds_eval_medium_rows = await map(ds_eval, mistral_medium.predict, col_name=\"mistral_medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval_medium = weave.Dataset(name=\"ds_eval_medium\", description=\"Mistral medium predictions\", rows=ds_eval_medium_rows)\n",
    "weave.publish(ds_eval_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval_medium = weave.ref('ds_eval_medium:latest').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_7b = MistralModel(model=\"open-mistral-7b\")\n",
    "ds_eval_7b_rows = await map(ds_eval_medium, mistral_7b.predict, col_name=\"mistral_7b\")\n",
    "ds_eval_7b_medium = weave.Dataset(name=\"ds_eval_medium_7b\", description=\"Mistral 7b predictions along with medium\", rows=ds_eval_7b_rows)\n",
    "weave.publish(ds_eval_7b_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge(weave.Model):\n",
    "    model: str = \"mistral-large-latest\"\n",
    "    \n",
    "    @weave.op\n",
    "    async def predict(self, keyword: str, mistral_7b: str, mistral_medium: str, text: str, **kwargs) -> dict:\n",
    "        messages = [\n",
    "            ChatMessage(\n",
    "                role=\"user\",\n",
    "                content=(\n",
    "                \"You are a world class comedian and you are judging a joke competition in Chile.\"\n",
    "                \"You have to pick the best joke between two jokes written about a keyword.\"\n",
    "                \"Take into consideration the jokes were written in Chilean Spanish and a ground truth joke as a reference. \\n\"\n",
    "                \"Here is the keyword: {keyword}\\n\"\n",
    "                \"Here is the joke1: {mistral_7b}\\n\"\n",
    "                \"Here is the joke2: {mistral_medium}\\n\"\n",
    "                \"Ground truth joke: {joke}\\n\"\n",
    "                \"Return the name of the best_joke (or None if you think both are bad) and the reason in short JSON object.\").format(\n",
    "                    keyword=keyword, \n",
    "                    mistral_7b=mistral_7b, \n",
    "                    mistral_medium=mistral_medium,\n",
    "                    joke=text)\n",
    "            )\n",
    "        ]\n",
    "        payload = await call_mistral(model=self.model, messages=messages, response_format={\"type\": \"json_object\"})\n",
    "        return json.loads(payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval_7b_medium.rows[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_judge = LLMJudge()\n",
    "res = await llm_judge.predict(**ds_eval_7b_medium.rows[0])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@weave.op\n",
    "def evaluate_joke(model_output: str) -> dict:\n",
    "    \"Evaluate the answer\"\n",
    "    return {\"win\": model_output[\"best_joke\"] == \"joke1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(dataset=ds_eval_7b_medium, scorers=[evaluate_joke])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluation.evaluate(llm_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_messages(row):\n",
    "    \"Format on the expected MistralAI fine-tuning dataset\"\n",
    "    keyword = row['keyword']\n",
    "    joke = row['text']\n",
    "    messages = create_messages(keyword, cls=dict)\n",
    "    # we need to append the answer for training ðŸ‘‡\n",
    "    messages = {\"messages\":messages + [dict(role=\"assistant\", content=joke)]}\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../data/processed/jokes.jsonl', lines=True)\n",
    "df_train=df.sample(frac=0.95, random_state=200)\n",
    "df_eval=df.drop(df_train.index)\n",
    "len(df_train), len(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df_train = df_train.apply(format_messages, axis=1)\n",
    "formatted_df_eval = df_eval.apply(format_messages, axis=1)\n",
    "formatted_df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_df_train.to_json(\"../data/processed/formatted_df_train.jsonl\", orient=\"records\", lines=True)\n",
    "formatted_df_eval.to_json(\"../data/processed/formatted_df_eval.jsonl\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralClient(api_key=os.environ[\"MISTRAL_API_KEY\"])\n",
    "\n",
    "with open(\"../data/processed/formatted_df_train.jsonl\", \"rb\") as f:\n",
    "    ds_train = client.files.create(file=(\"formatted_df_train.jsonl\", f))\n",
    "with open(\"../data/processed/formatted_df_eval.jsonl\", \"rb\") as f:\n",
    "    ds_eval = client.files.create(file=(\"eval.jsonl\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(obj):\n",
    "    print(json.dumps(obj.dict(), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "created_jobs = client.jobs.create(\n",
    "    model=\"open-mistral-7b\",\n",
    "    training_files=[ds_train.id],\n",
    "    validation_files=[ds_eval.id],\n",
    "    hyperparameters=TrainingParameters(\n",
    "        training_steps=25,\n",
    "        learning_rate=0.0001,\n",
    "        ),\n",
    "    integrations=[\n",
    "        WandbIntegrationIn(\n",
    "            project=\"mistral_hackathon\",\n",
    "            run_name=\"finetune_wandb\",\n",
    "            api_key=os.environ.get(\"WANDB_API_KEY\"),\n",
    "        ).dict()\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(created_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
    "while retrieved_job.status in [\"RUNNING\", \"QUEUED\"]:\n",
    "    retrieved_job = client.jobs.retrieve(created_jobs.id)\n",
    "    pprint(retrieved_job)\n",
    "    print(f\"Job is {retrieved_job.status}, waiting 10 seconds\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_jobs = client.jobs.retrieve(created_jobs.id)\n",
    "pprint(retrieved_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_eval_medium = weave.ref('ds_eval_medium:latest').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MistralAsyncClient(api_key=os.environ[\"MISTRAL_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_7b_ft = MistralModel(model=retrieved_jobs.fine_tuned_model)\n",
    "ds_eval_7b_rows = await map(ds_eval_medium, mistral_7b_ft.predict, col_name=\"mistral_7b\")\n",
    "ds_eval_7b_ft_medium = weave.Dataset(name=\"ds_eval_medium_7b_ft\", description=\"Finetuned Mistral 7b predictions along with medium\", rows=ds_eval_7b_rows)\n",
    "weave.publish(ds_eval_7b_ft_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = weave.Evaluation(dataset=ds_eval_7b_ft_medium, scorers=[evaluate_joke])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await evaluation.evaluate(llm_judge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
